{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "561b9909",
   "metadata": {},
   "source": [
    "# StackGAN CUB200 DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33afe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "import torch.utils.data as data\n",
    "import pickle\n",
    "from torchvision.utils import make_grid\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325507ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, dataDir, split='train', imgSize= imgSize, transform=None):\n",
    "\n",
    "        super(LoadDataset,self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        # Normalize the image\n",
    "        self.norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        \n",
    "        self.imgSize = imgSize\n",
    "        self.dataDir = dataDir\n",
    "        self.filenames, self.caps = self.load_info(dataDir, split)\n",
    "        self.bbox = self.load_bbox()\n",
    "        self.classes = self.load_class(dataDir, split)\n",
    "              \n",
    "    def load_info(self, dataDir, split):\n",
    "        filenames = self.load_filenames(dataDir, split)\n",
    "        captionFile = os.path.join(dataDir, 'birds', split, 'char-CNN-RNN-embeddings.pickle')\n",
    "        with open (captionFile, 'rb') as f:\n",
    "            captions = np.array(captions)\n",
    "        \n",
    "        return filenames, captions\n",
    "        \n",
    "    def load_filenames(self, dataDir, split):\n",
    "        path = os.path.join(dataDir, 'birds', split, 'filenames.pickle')\n",
    "        with open(path, 'rb') as f:\n",
    "            filenames = pickle.load(f,encoding='latin1')\n",
    "        return filenames\n",
    "    \n",
    "    def load_bbox(self):\n",
    "        path = os.path.join(self.dataDir,'birds','CUB_200_2011', 'bounding_boxes.txt')\n",
    "        bbox_data = pd.read_csv(path, delim_whitespace=True, header=None).astype(int)\n",
    "\n",
    "        filepath = os.path.join(self.dataDir,'birds', 'CUB_200_2011','images.txt')\n",
    "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
    "        filenames = sorted( list(df_filenames[1]))\n",
    "        fname_bbox_dict = {x[:-4]:[] for x in filenames} \n",
    "        for i in range(len(filenames)):\n",
    "            data = list(bbox_data.iloc[1][1:])\n",
    "            k = filenames[i][:-4]\n",
    "            fname_bbox_dict[k] = data\n",
    "        return fname_bbox_dict\n",
    "    \n",
    "    def load_class(self, dataDir, split):\n",
    "        path = os.path.join(dataDir, 'birds', split, 'class_info.pickle')\n",
    "        if os.path.isfile(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                classId = pickle.load(f,encoding='latin1')\n",
    "        else:\n",
    "            classId = np.arange(len(self.filenames))\n",
    "        return classId\n",
    "\n",
    "    def get_img(self, img_path, bbox=None):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.size\n",
    "        if bbox is not None:\n",
    "            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "            y1 = np.maximum(0, center_y - R)\n",
    "            y2 = np.minimum(height, center_y + R)\n",
    "            x1 = np.maximum(0, center_x - R)\n",
    "            x2 = np.minimum(width, center_x + R)\n",
    "            img = img.crop([x1, y1, x2, y2])\n",
    "        load_size = int(self.imgSize * 76 / 64)\n",
    "        img = img.resize((load_size, load_size), Image.BILINEAR)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.filenames[idx]\n",
    "        \n",
    "        if self.bbox is not None:\n",
    "            bbox = self.bbox[key]\n",
    "        else:\n",
    "            bbox = None\n",
    "        emb = self.caps[idx, :, :]\n",
    "        imagePath = os.path.join(self.dataDir,'birds', 'CUB_200_2011', 'images',self.filenames[idx]+'.jpg')\n",
    "        image = self.get_img(imagePath, bbox)\n",
    "        \n",
    "        # random select a sentence\n",
    "        sample = np.random.randint(0, emb.shape[0]-1)\n",
    "        cap = emb[sample, :]\n",
    "        return image, cap\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initateWeights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# using APPLE M2 Chip       \n",
    "CUDA = False\n",
    "cond_dim = 128\n",
    "df_dim = 128\n",
    "gf_dim = 128\n",
    "z_dim = 100\n",
    "emb_dim = 1024\n",
    "\n",
    "def Conv_k3(in_p, out_p, stride=1):\n",
    "    return nn.Conv2d(in_p, out_p, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class UpperBlock(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super(UpperBlock, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv = Conv_k3(inp, outp)\n",
    "        self.batch = nn.BatchNorm2d(outp)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = self.up(x)\n",
    "        o = self.relu(self.conv(o))\n",
    "        o = self.batch(o)\n",
    "        return o\n",
    "\n",
    "class DiscriminatorOutput(nn.Module):\n",
    "    def __init__(self, have_cond = True):\n",
    "        super(DiscriminatorOutput, self).__init__()\n",
    "        self.have_cond = have_cond\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        if have_cond:\n",
    "            cond_part = nn.Sequential(\n",
    "                Conv_k3(in_p=1024+128, out_p=1024),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "            self.classifier = torch.nn.Sequential(*(list(cond_part)+list(self.classifier)))\n",
    "        print(self.classifier)\n",
    "            \n",
    "    def forward(self, encoded_image, encoded_cond=None):\n",
    "        if self.have_cond and encoded_cond is not None:\n",
    "            cond = encoded_cond.view(-1, 128 , 1, 1)\n",
    "            cond = cond.repeat(1, 1, 4, 4)\n",
    "            image_with_cond = torch.cat((encoded_image, cond), 1)\n",
    "        else:\n",
    "            image_with_cond = encoded_image\n",
    "        return self.classifier(image_with_cond).view(-1)\n",
    "\n",
    "class CondArgumentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CondArgumentModel,self).__init__()\n",
    "        self.fc = nn.Linear(in_features=emb_dim, out_features=cond_dim*2)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def convert(self, embed):\n",
    "        x = self.relu(self.fc(embed))\n",
    "        mean, sigma = x[:, : cond_dim], x[:, cond_dim:]\n",
    "        return mean, sigma\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, sigma = self.convert(x)\n",
    "        diag = torch.exp(sigma*0.5)\n",
    "        if CUDA:\n",
    "            normal_dis = (torch.FloatTensor(diag.size()).normal_()).cuda()\n",
    "        else:\n",
    "            normal_dis = (torch.FloatTensor(diag.size()).normal_())\n",
    "        condition = (diag*normal_dis)+mean\n",
    "        return condition, mean, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b779007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Stage_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generate_Stage_1, self).__init__()\n",
    "        self.CA = CondArgumentModel()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=228, out_features=128*8*4*4, bias=False),\n",
    "            nn.BatchNorm1d(128*8*4*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.img = nn.Sequential(\n",
    "            UpperBlock(128*8,64*8),\n",
    "            UpperBlock(64*8,32*8),\n",
    "            UpperBlock(32*8,16*8),\n",
    "            UpperBlock(16*8,8*8),\n",
    "            Conv_k3(8*8, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, emb):\n",
    "        cond, mean, sigma = self.CA(emb)\n",
    "        cond = cond.view(noise.size(0), cond_dim, 1, 1)\n",
    "        x = torch.cat((noise, cond),1)\n",
    "        x = x.view(-1, 228)\n",
    "        o = self.fc(x)\n",
    "        h_code = o.view(-1, 128*8, 4, 4)\n",
    "        fake_img = self.img(h_code)\n",
    "        return fake_img, mean, sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3981f73b",
   "metadata": {},
   "source": [
    "### Building  Stage - 1 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorStage1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorStage1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            #c alucalation output size = [(input_size −Kernal +2Padding )/Stride ]+1\n",
    "            # input is image 3 x 64 x 64  \n",
    "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),# => 128 x 32 x 32 \n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),# => 256 x 16 x 16\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),# => 512 x 8 x 8\n",
    "            \n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True)# => 1024 x 4 x 4\n",
    "        )\n",
    "        self.condition_classifier = DiscriminatorOutput()\n",
    "        self.uncondition_classifier = None\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(mean, sigma):\n",
    "        temp = 1+sigma+((-1)*((mean*mean)+sigma))\n",
    "        return torch.mean(temp)*(-0.5)\n",
    "\n",
    "def CalculateGeneratorLoss(net_dim, fake_images, y_labels, condition):\n",
    "    criterion = nn.BCELoss()\n",
    "    cond = cond.detach()\n",
    "    fake_f = net_dim(fake_images)\n",
    "\n",
    "    fake_cond_ouput = net_dim.condition_classifier(fake_f, condition)\n",
    "    error_fakes = criterion(fake_cond_ouput, y_labels)\n",
    "    if net_dim.uncondition_classifier is not None:\n",
    "        fake_uncond_output = net_dim.uncondition_classifier(fake_f)\n",
    "        uncond_errD_fake = criterion(fake_uncond_output, y_labels)\n",
    "        errD_fake += uncond_errD_fake\n",
    "    return error_fakes\n",
    "\n",
    "def calculate_discriminator_loss(net_dis, real_images, fake_images, y_real, y_fake, condition):\n",
    "    criterion = nn.BCELoss()\n",
    "    batch_size = real_images.size(0)\n",
    "    cond = cond.detach()\n",
    "    fake = fake_images.detach()\n",
    "\n",
    "    real_img_feature = net_dis(real_images)\n",
    "    fake_img_feature = net_dis(fake)\n",
    "\n",
    "    real_output = net_dis.condition_classifier(real_img_feature, cond)\n",
    "    errD_real  = criterion(real_output, y_real)\n",
    "    wrong_output = net_dis.condition_classifier(real_img_feature[:(batch_size-1)], cond[1:])\n",
    "    errD_wrong = criterion(wrong_output, y_fake[1:])\n",
    "\n",
    "    fake_output = net_dis.condition_classifier(fake_img_feature, cond)\n",
    "    errD_fake= criterion(fake_output, y_fake)\n",
    "\n",
    "    if net_dis.uncondition_classifier is not None:\n",
    "        real_uncond_output = net_dis.uncondition_classifier(real_img_feature)\n",
    "        errD_real_uncond = criterion(real_uncond_output, y_real)\n",
    "\n",
    "        fake_uncond_output = net_dis.uncondition_classifier(fake_img_feature)\n",
    "        errD_fake_uncond = criterion(fake_uncond_output, y_fake)\n",
    "\n",
    "        error_detected = (errD_real+errD_real_uncond)/2. + (errD_fake+errD_wrong+errD_fake_uncond)/3.\n",
    "        errD_real =  (errD_real+errD_real_uncond)/2\n",
    "        errD_fake = (errD_fake+errD_fake_uncond)/2.\n",
    "    else:\n",
    "        errD = errD_real + (errD_fake+errD_wrong)*0.5\n",
    "    return errD, errD_real.item(), errD_wrong.item(), errD_fake.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf9606f",
   "metadata": {},
   "source": [
    "### Training Stage - 1 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504e8fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/donnaphat-ut/StackGAN\n",
    "    \n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 64\n",
    "    transform = transforms.Compose([\n",
    "                transforms.RandomCrop(64),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    dataset = LoadDataset(dataDir = 'Datasets/', split='train', transform=transform)\n",
    "    loaded_datasets = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    netG = Generate_Stage_1().to(device)\n",
    "    netG.apply(initateWeights)\n",
    "    netD = DiscriminatorStage1().to(device)\n",
    "    netD.apply(initateWeights)\n",
    "    lr = 0.0002\n",
    "    optD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.rand(batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "    real_labels = (torch.FloatTensor(batch_size).fill_(1)).to(device)\n",
    "    fake_labels = (torch.FloatTensor(batch_size).fill_(0)).to(device)\n",
    "    \n",
    "    num_epoch = 600\n",
    "    iters = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            lr = lr*0.5\n",
    "            for param_group in optG.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            for param_group in optD.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        for i, data in enumerate(loaded_datasets,0):\n",
    "            real_imgs, encoded_caps = data\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            encoded_caps = encoded_caps.to(device)\n",
    "\n",
    "            ##update discriminator\n",
    "            netD.zero_grad()\n",
    "            # generate fake image\n",
    "            noise = torch.rand(batch_size, z_dim, 1, 1).to(device)\n",
    "            fake_imgs, m, s = netG(noise, encoded_caps)\n",
    "            errD, errD_real, errD_wrong, errD_fake = calculate_discriminator_loss(netD, real_imgs, fake_imgs, real_labels, fake_labels, m)\n",
    "            errD.backward()\n",
    "            optD.step()\n",
    "\n",
    "            ##update generator\n",
    "            netG.zero_grad()\n",
    "            errG = CalculateGeneratorLoss(netD, fake_imgs, real_labels, m)\n",
    "            errG += errG + KL_loss(m,s)\n",
    "            errG.backward()\n",
    "            optG.step()\n",
    "            \n",
    "            iters+=1\n",
    "\n",
    "            if i%50 == 0:\n",
    "                 print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLoss_D_R: %.4f\\tLoss_D_W: %.4f\\tLoss_D_F %.4f'\n",
    "                      % (epoch, num_epoch, i, len(loaded_datasets),\n",
    "                         errD.item(), errG.item(), errD_real, errD_wrong, errD_fake))\n",
    "        if epoch%50==0:\n",
    "            with torch.no_grad():\n",
    "                fake, _, _  = netG(fixed_noise, encoded_caps)\n",
    "                # fig = plt.figure(figsize=(10,10))\n",
    "                grid = make_grid(fake.detach().cpu(), nrow=8, normalize=True).permute(1,2,0).numpy()\n",
    "                plt.imshow(grid)\n",
    "                # fig.savefig('results1/epch-{}.png'.format(epoch))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15827906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, plane):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            Conv_k3(plane, plane),\n",
    "            nn.BatchNorm2d(plane),\n",
    "            nn.ReLU(True),\n",
    "            Conv_k3(plane, plane),\n",
    "            nn.BatchNorm2d(plane)\n",
    "        )\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tmp = x\n",
    "        o = self.block(x)\n",
    "        o = o + tmp\n",
    "        return self.relu(o)\n",
    "    \n",
    "class G_Stage2(nn.Module):\n",
    "    def __init__(self, G_Stage1):\n",
    "        super(G_Stage2, self).__init__()\n",
    "        self.G1 = G_Stage1\n",
    "        self.CA = CondAugment_Model()\n",
    "        for p in self.G1.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.encoder = nn.Sequential(\n",
    "            Conv_k3(3, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128 * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128 * 2, 128 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128 * 4),\n",
    "            nn.ReLU(True))\n",
    "        self.combine = nn.Sequential(\n",
    "            Conv_k3(640, 512),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            ResBlock(512),\n",
    "            ResBlock(512),\n",
    "            ResBlock(512),\n",
    "            ResBlock(512)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            Upblock(512,256),\n",
    "            Upblock(256,128),\n",
    "            Upblock(128,64),\n",
    "            Upblock(64,32),\n",
    "            Conv_k3(32,3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, emb):\n",
    "        init_image, _, _ = self.G1(noise, emb)\n",
    "        encoded = self.encoder(init_image)\n",
    "        \n",
    "        cond, m, s = self.CA(emb)\n",
    "        cond = cond.view(-1, 128, 1, 1)\n",
    "        cond = cond.repeat(1, 1, 16, 16)\n",
    "        \n",
    "        encoded_cond = torch.cat([encoded, cond],1)\n",
    "        img_feature = self.combine(encoded_cond)\n",
    "        img_feature = self.residual(img_feature)\n",
    "        img = self.decoder(img_feature)\n",
    "        \n",
    "        return init_image, img, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Stage2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_Stage2, self).__init__()\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            # start 3 x 256 x 256\n",
    "            nn.Conv2d(3, 128, 4, 2, 1, bias=False), #=> 128 x 128 x 128\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False), #=> 256 x 64 x 64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False), #=> 512 x 32 x 32\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False), #=> 1024 x 16 x 16\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(1024, 2048, 4, 2, 1, bias=False), #=> 2048 x 8 x 8\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(2048, 4096, 4, 2, 1, bias=False), #=> 4096 x 4 x 4\n",
    "            nn.BatchNorm2d(4096),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            Conv_k3(4096, 2048), # 2048 x 4 x 4\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            Conv_k3(2048, 1024), # 1024 x 4 x 4\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.condition_classifier = DiscriminatorOutput()\n",
    "        self.uncondition_classifier = DiscriminatorOutput(have_cond=False)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img_feature = self.img_encoder(img)\n",
    "        return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3846167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda:0')\n",
    "    # load dataset with size 256x256\n",
    "    batch_size = 55\n",
    "    transform = transforms.Compose([\n",
    "                transforms.RandomCrop(256),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    \n",
    "    dataset = LoadDataset(dataDir = 'Datasets/', split='train', transform=transform, imgSize=256)\n",
    "    tr_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    #load model Stage-I generator and put it into Stage-II generator\n",
    "    G1 = Generate_Stage_1()\n",
    "    G1.eval()\n",
    "    netG = G_Stage2(G1).to(device)\n",
    "    netG.apply(initateWeights)\n",
    "    netD = D_Stage2().to(device)\n",
    "    netD.apply(initateWeights)\n",
    "\n",
    "    lr = 0.0002\n",
    "    optD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    # remove the parameter from Stage-I generator\n",
    "    netG_param = []\n",
    "    for p in netG.parameters():\n",
    "        if p.requires_grad:\n",
    "            netG_param.append(p)\n",
    "    optG = optim.Adam(netG_param, lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.rand(batch_size, 100, 1, 1).to(device)\n",
    "\n",
    "    real_labels = (torch.FloatTensor(batch_size).fill_(1)).to(device)\n",
    "    fake_labels = (torch.FloatTensor(batch_size).fill_(0)).to(device)\n",
    "    \n",
    "    num_epoch = 45\n",
    "    iters = 0\n",
    "    for epoch in range(num_epoch+1):\n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            lr = lr*0.5\n",
    "            for param_group in optG.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            for param_group in optD.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        for i, data in enumerate(tr_loader,0):\n",
    "            real_imgs, encoded_caps = data\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            encoded_caps = encoded_caps.to(device)\n",
    "\n",
    "            ##update discriminator\n",
    "            netD.zero_grad()\n",
    "            # generate fake image\n",
    "            noise = torch.rand(batch_size, 100, 1, 1).to(device)\n",
    "            init_img ,fake_imgs, m, s = netG(noise, encoded_caps)\n",
    "            errD, errD_real, errD_wrong, errD_fake = calculate_discriminator_loss(netD, real_imgs, fake_imgs, real_labels, fake_labels, m)\n",
    "            errD.backward()\n",
    "            optD.step()\n",
    "\n",
    "            ##update generator\n",
    "            netG.zero_grad()\n",
    "            errG = CalculateGeneratorLoss(netD, fake_imgs, real_labels, m)\n",
    "            errG += errG + KL_loss(m,s)\n",
    "            errG.backward()\n",
    "            optG.step()     \n",
    "            \n",
    "            if i%50 == 0:\n",
    "                 print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLoss_D_R: %.4f\\tLoss_D_W: %.4f\\tLoss_D_F %.4f'\n",
    "                      % (epoch, num_epoch, i, len(tr_loader),\n",
    "                         errD.item(), errG.item(), errD_real, errD_wrong, errD_fake))\n",
    "        if epoch%10==0:\n",
    "            with torch.no_grad():\n",
    "                _, fake, _, _  = netG(fixed_noise, encoded_caps)\n",
    "                fig = plt.figure(figsize=(10,10))\n",
    "                grid = make_grid(fake.detach().cpu(), nrow=8, normalize=True).permute(1,2,0).numpy()\n",
    "                plt.imshow(grid)\n",
    "                \n",
    "        if epoch%15==0:\n",
    "            torch.save(netG.state_dict(), 'results2/netG2_epoch_{}.pth'.format(epoch))\n",
    "    torch.save(netD.state_dict(), 'results2/netD2_epoch_last.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41082e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "G1 = Generate_Stage_1().to(device)\n",
    "G1checkpoint = torch.load('results1/netG_epoch_600.pth')\n",
    "G1.load_state_dict(G1checkpoint)\n",
    "G1.eval()\n",
    "\n",
    "netG = G_Stage2(G1).to(device)\n",
    "G2checkpoint = torch.load('results2/netG2_epoch_450.pth')\n",
    "netG.load_state_dict(G2checkpoint)\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.RandomCrop(256),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "\n",
    "    \n",
    "dataset = LoadDataset(dataDir = 'Datasets/', split='test', transform=transform, imgSize=256)\n",
    "te_loader = DataLoader(dataset, batch_size= 55, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc674eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "te_loader = iter(te_loader)\n",
    "imgTensor, captions = next(te_loader)\n",
    "\n",
    "batch_size = 55\n",
    " \n",
    "with torch.no_grad():# Generate image grid\n",
    "    grid1 = make_grid(imgTensor[:10], padding = 4, nrow=10)\n",
    "    grid1 = grid1.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(20, 8), dpi=300)\n",
    "    plt.imshow(grid1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.rand(batch_size, z_dim, 1, 1).to(device)\n",
    "    cap = captions.to(device)\n",
    "    init_imgs, fake_imgs, m, s = netG(noise, cap)\n",
    "\n",
    "    grid2 = make_grid(fake_imgs.detach().cpu()[:10], padding = 4, nrow=10).permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(20, 8), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(grid2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
